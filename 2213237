#include <stdio.h>
#include <stdlib.h>
#include <omp.h>

void bubbleSort(int* array, int n) {
    int i, j, temp;
    int swapped;

    for (i = 0; i < n - 1; i++) {
        swapped = 0;
        
        #pragma omp parallel for private(j, temp) shared(array, swapped)
        for (j = 0; j < n - i - 1; j++) {
            if (array[j] > array[j + 1]) {
                // Swap the elements
                temp = array[j];
                array[j] = array[j + 1];
                array[j + 1] = temp;
                swapped = 1;
            }
        }

        // If no two elements were swapped by inner loop, then break
        if (swapped == 0)
            break;
    }
}

void printArray(int* array, int size) {
    for (int i = 0; i < size; i++) {
        printf("%d ", array[i]);
    }
    printf("\n");
}

int main() {
    int n;
    printf("Enter number of elements: ");
    scanf("%d", &n);

    int* array = (int*)malloc(n * sizeof(int));
    if (array == NULL) {
        printf("Memory allocation failed\n");
        return -1;
    }

    printf("Enter elements:\n");
    for (int i = 0; i < n; i++) {
        scanf("%d", &array[i]);
    }

    bubbleSort(array, n);

    printf("Sorted array: \n");
    printArray(array, n);

    free(array);
    return 0;
}


OpenMP Overview
OpenMP (Open Multi-Processing) is an API that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran. It provides a simple and flexible interface for developing parallel applications.

Key Concepts:

Directives: OpenMP uses compiler directives to parallelize the code. These directives are special pragmas that the compiler recognizes to generate parallel code.

#pragma omp parallel: This directive starts a parallel region.
#pragma omp for: This directive is used to parallelize loops.
Clauses: OpenMP directives can have clauses to specify the behavior of variables in parallel regions.

private(variable): Each thread has its own instance of the variable.
shared(variable): All threads share the same instance of the variable.
reduction(operator:variable): Performs a reduction operation (like sum, max) on the variable across all threads.
Environment Variables: OpenMP uses environment variables to control the execution of parallel code.

OMP_NUM_THREADS: Specifies the number of threads to use in parallel regions.
Example:

c
Copy code
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel
    {
        printf("Hello from thread %d\n", omp_get_thread_num());
    }
    return 0;
}
This simple program prints a message from each thread in a parallel region. omp_get_thread_num() returns the thread number within the parallel region.
Compilation:

To compile an OpenMP program, use the -fopenmp flag with the compiler.
sh
Copy code
gcc -fopenmp -o program program.c
Summary
The provided code demonstrates a bubble sort algorithm parallelized with OpenMP.
OpenMP simplifies parallel programming with compiler directives and environment variables.
Parallel regions can be created using #pragma omp parallel and loops can be parallelized using #pragma omp parallel for.
Proper use of private, shared, and other clauses ensures correct variable handling across threads.
