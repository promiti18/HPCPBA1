#include <stdio.h>
#include <stdlib.h>
#include <omp.h>

void bubbleSort(int* array, int n) {
    int i, j, temp;
    int swapped;

    for (i = 0; i < n - 1; i++) {
        swapped = 0;
        
        #pragma omp parallel for private(j, temp) shared(array, swapped)
        for (j = 0; j < n - i - 1; j++) {
            if (array[j] > array[j + 1]) {
                // Swap the elements
                temp = array[j];
                array[j] = array[j + 1];
                array[j + 1] = temp;
                swapped = 1;
            }
        }

        // If no two elements were swapped by inner loop, then break
        if (swapped == 0)
            break;
    }
}

void printArray(int* array, int size) {
    for (int i = 0; i < size; i++) {
        printf("%d ", array[i]);
    }
    printf("\n");
}

int main() {
    int n;
    printf("Enter number of elements: ");
    scanf("%d", &n);

    int* array = (int*)malloc(n * sizeof(int));
    if (array == NULL) {
        printf("Memory allocation failed\n");
        return -1;
    }

    printf("Enter elements:\n");
    for (int i = 0; i < n; i++) {
        scanf("%d", &array[i]);
    }

    bubbleSort(array, n);

    printf("Sorted array: \n");
    printArray(array, n);

    free(array);
    return 0;
}


OpenMP Overview
OpenMP (Open Multi-Processing) is an API that supports multi-platform shared memory multiprocessing programming in C, C++, and Fortran. It provides a simple and flexible interface for developing parallel applications.

Key Concepts:

Directives: OpenMP uses compiler directives to parallelize the code. These directives are special pragmas that the compiler recognizes to generate parallel code.

#pragma omp parallel: This directive starts a parallel region.
#pragma omp for: This directive is used to parallelize loops.
Clauses: OpenMP directives can have clauses to specify the behavior of variables in parallel regions.

private(variable): Each thread has its own instance of the variable.
shared(variable): All threads share the same instance of the variable.
reduction(operator:variable): Performs a reduction operation (like sum, max) on the variable across all threads.
Environment Variables: OpenMP uses environment variables to control the execution of parallel code.

OMP_NUM_THREADS: Specifies the number of threads to use in parallel regions.
Example:

c
Copy code
#include <omp.h>
#include <stdio.h>

int main() {
    #pragma omp parallel
    {
        printf("Hello from thread %d\n", omp_get_thread_num());
    }
    return 0;
}
This simple program prints a message from each thread in a parallel region. omp_get_thread_num() returns the thread number within the parallel region.

Parallelizing Bubble Sort with OpenMP
Bubble sort is a simple sorting algorithm that repeatedly steps through the list, compares adjacent elements, and swaps them if they are in the wrong order. The pass through the list is repeated until the list is sorted. While bubble sort is not the most efficient sorting algorithm, it serves as a useful example for understanding parallelism.

Why Bubble Sort is Challenging to Parallelize
The bubble sort algorithm inherently involves comparisons and swaps of adjacent elements, making it highly sequential:

Each pass through the array moves the next largest element to its correct position.
The comparisons and swaps in one iteration depend on the results of the previous iteration.
This dependency makes it difficult to parallelize the bubble sort efficiently. However, for the sake of learning, we can attempt to parallelize it in a way that might offer some performance improvements.

Basic Parallelization Using OpenMP
In the provided example, OpenMP is used to parallelize the inner loop of the bubble sort. Here's a breakdown of how it's done and some theoretical considerations:

1. Parallelizing the Inner Loop
In the inner loop, we compare and possibly swap adjacent elements. By using OpenMP's #pragma omp parallel for directive, we attempt to parallelize these comparisons and swaps.

Code Segment:
c
Copy code
#pragma omp parallel for private(j, temp) shared(array, swapped)
for (j = 0; j < n - i - 1; j++) {
    if (array[j] > array[j + 1]) {
        temp = array[j];
        array[j] = array[j + 1];
        array[j + 1] = temp;
        swapped = 1;
    }
}
#pragma omp parallel for: This directive creates a parallel region and distributes the iterations of the for loop among the threads in the parallel region.
private(j, temp): Specifies that each thread should have its own instances of j and temp.
shared(array, swapped): Specifies that array and swapped are shared among all threads.
2. Issues with Basic Parallelization
The basic parallelization of the inner loop introduces some potential issues:

Race Conditions: Multiple threads might attempt to swap elements simultaneously, leading to inconsistent states if not handled correctly.
Limited Parallelism: Due to the dependency on adjacent elements, the opportunities for parallelism are limited, especially in the later stages of the sorting process where fewer elements remain to be compared.
Advanced Parallelization Techniques
To overcome the limitations of basic parallelization, more sophisticated techniques can be applied. Here are a few approaches:

1. Odd-Even Transposition Sort
Odd-even transposition sort is a parallel sorting algorithm that is a variant of bubble sort. It works as follows:

In the odd phase, compare and swap elements at odd indices with their next elements.
In the even phase, compare and swap elements at even indices with their next elements.
Repeat the odd and even phases until the array is sorted.
Code Example:
c
Copy code
void oddEvenSort(int* array, int n) {
    int sorted = 0;
    while (!sorted) {
        sorted = 1;

        #pragma omp parallel for
        for (int i = 1; i < n - 1; i += 2) {
            if (array[i] > array[i + 1]) {
                int temp = array[i];
                array[i] = array[i + 1];
                array[i + 1] = temp;
                sorted = 0;
            }
        }

        #pragma omp parallel for
        for (int i = 0; i < n - 1; i += 2) {
            if (array[i] > array[i + 1]) {
                int temp = array[i];
                array[i] = array[i + 1];
                array[i + 1] = temp;
                sorted = 0;
            }
        }
    }
}
This approach ensures that comparisons are done on independent elements, allowing better parallelism.
2. Reducing Synchronization Overhead
Chunking: Divide the array into chunks and sort each chunk in parallel. Merge the chunks in subsequent steps.
Atomic Operations: Use OpenMP atomic operations to ensure thread-safe updates to shared variables.
Considerations and Best Practices
Thread Management: Efficiently manage the number of threads to avoid overheads. The environment variable OMP_NUM_THREADS can be set to control the number of threads.

Load Balancing: Ensure that work is evenly distributed among threads to avoid idle threads. This can be achieved by dynamically scheduling the loop iterations using the schedule clause in OpenMP.

Synchronization: Minimize the use of synchronization primitives like critical sections and barriers to reduce overhead.

Memory Access Patterns: Optimize memory access patterns to improve cache performance. Contiguous memory access is generally faster due to better cache utilization.

Conclusion
Parallelizing bubble sort using OpenMP provides a practical example to understand the basics of parallel programming. While bubble sort is not ideal for parallelization due to its inherent sequential nature, more advanced techniques like odd-even transposition sort can improve parallel performance. OpenMP simplifies the implementation of parallelism with its pragmas and clauses, making it a powerful tool for developing parallel applications. Understanding these concepts can be extended to more complex and efficient sorting algorithms for real-world applications.

